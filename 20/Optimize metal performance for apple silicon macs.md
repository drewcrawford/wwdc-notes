#metal #applesilicon 
# Apple silicon mac
* faster
* more powe refficient
* rich feature set
* different architecture
* familiar best practices

# Transition process

Note that we are dealing with the case that your mac app is already native on arm64 and you're looking for more performance

# TBDR review
GPU rasterizes all geometry before drawing pixels


# Performance best practices for apple GPUs

## Schedule work efficiently
Unnecessary dependencies across render passes create serialization.

3 execution chanenls  â€“ vertex, fragment, compute
Overlap whenever possible

Data dependencies are a natural result of producing/consuming metal resources between passes.

Note that vertex processes of the next pass should overlap with fragment passes of the current pass.  Ideally, vertex work is hidden by the fragment work.  However, vertex passes that depend on a fragment pass (the prior pass) creates a dependency.

False dependency - sharing the same resources between adjacent passes, but not the data inside those resources.

Options include
* Using 2 (or however-many) to sort of "double-buffer" your buffers.  In this case metal understands there's not a dependency
* Using `untracked` and then fences/events when synchronization is required

More on fences/events:
[[what's new in metal, part 1 - 16]]
[[metal for game developers - 18]]

## Pass reordering
Encode independent work as early as possible.

Note that metal can reorder independent passes when it can, but metal's visibility is limited, so prefer reordering explicitly.

### Metal system trace
Will clearly highlight missed opportunities (bubbles)

## Minimize system bandwidth

Apple GPUs process render pass attachments in tile storage.
Load/store actiosn consume system bandwidth.
GPUs achieve peak performance when not ssytem bandwidth bound, so minimize load/store traffic.

Think about the case of trying to encode in parallel.  You may set up multiple command buffers.

This can lead to the situation where you have multiple render passes, that operate on the same resources.  However, the load/store actions for all these passes incur memory bandwidth.  Ideally we want to merge these passes so we aren't loading/storing intermediate stuff.

Instead, use "parallel render command encoder".  You still achieve your goal of multithreaded encoding, but without the execution cost.  sub-encoders.

`commandBuffer.makeParallelRendercommandEncoder(...)`

`parallelEncoder.makeRenderCommandEncoder` <- subencoder

Create these upfront ot match the intended order.

Then hand off subencoders to threads.

Once encoding is complete, finalize the encoder.

### Splits

Note that we don't need to split into multiple passes when only the load/store actions change.   This eliminates intermediate loads/stores.

### Pingponging

Idea here seems to be doing a series of red,green,red,green type buffer bindings.
Like if we're going to preprocess a light and then apply the light, preprocess the next light, apply the next light,e tc.

Instead, we can use multiple attachments.  You can have up to 8 attachments in a render pass, and we can do the attenuation in a memoryless way.

### Separate clears

Ideally we don't want a dedicated clear operation, that will create traffic just to clear a buffer.  Instead, fold the clear operation into the render pass that needs it.

In apple gpu, the clear load action only operates on tile memory.

### Separate MSAA resolves
Some apps first store the sample data, and then reload it in a resolve pass.  Sample data is rarely-needed offchip, so instead we should not store samples unless we need them.


## Minimize overdraw

HSR in hardware.  

* opaque framgnets.  These occlude everything behind them
* Feedback fragments.  These are fragments generated by shaders that contain `fragment_discard`, or generate a depth value.
* Translucent fragments.  These blend with their background.

To maximize HSR efficiency, draw each type together and in that order.  Opaque, feedback, translucent.  

1.  Opaque (can be any order)
2.  Feedback (draw front to back)
3.  Translucent (draw front to back)

### Opaque fragments

Any pixel beneath them can be eliminated prior to shading.
Note that some metal features make your stuff non-opaque, like alpha-blending and alpha-testing.  Some metal features will also reduce efficiency.  Be aware of performance problems in the features below:

#### Resource writes
Writes to metal buffers and textures (not attachments).
Metal is required to execute all fragment resource writes, even if they are occluded
If you can, use `[[early_fragment_tests]]` to maximize rejection.

#### Write masking
Write masking restricts which channels of a render pass attachmenta re updated, even when the shader writes all the channels.  e.g we can say 'only write red and green'

Metal must preserve the untouched channels for correctness, and that means shading underallpping fragments to generate those values.
All underlapping fragment channels are computed even if only some is needed.

Note that in addition to write masking in metal API cpu-side, you can also set it up in fragment functions themselves.  Fragment functions that don't write any channels of an attachemnt are also write-masking.

To fix this, write an unused thing to some constant (e.g. 0).  **Write to all render pass attachments to improve HSR efficiency.**

### Depth pre-pass
Technique to reduce overdraw in mac games.
Render twice:
* first to depth attachment
* Depth-test equal to shade

If you need visibility information for a particular technique, it's ok.  But if you only do it for performance, HSR serves the same purpose.

HSR achieves similar performance, without additional costs.  And it doesn't require depth attachment to be stored (or even have memory backing).  Also avoids z-fighting caused by different pipeline numerics [[Port your mac app to apple silicon]]

# Metal features for apple TBDR
## Optimized deferred shading
Rendering is split into 2 passes
1.  Scene geometry to compute per-pixel e.g. albedo, normals, roughness.  "G-buffer"
2.  G-buffer data is used to compute Lighting

Main idea is that decoupling geometry from lighting keeps shading under control.

What we want to do is reduce the number of memory passes.  Programmable blending can reduce bandwidth cost, and memoryless targets improves footprint.

Note that memory buffers in fragment shaders are expensive in apple gpus.  This is because they are required to flush tile memory all the way into system memory, which is not ideal.

### Complex deferred shading 

For example, you may have separate passes for hair, lighting, etc.

Metal supports up to 8 color attachments per render pass.  If your G-buffer layout is similar among all logical passes, you can keep everything in one pass using programmable blending.

## Mixing render and compute

Modern renderers often implement more sophisticated lighting pipelines.  Sometimes they use both render-based techniques and compute dispatches.  e.g., tile-based light culling.  A compute shader builds a list of lights that affect each tile.

Keeping data on-chip requires additional work.

Starting with a11 bionic, you can do a tile-based compute dispatch.  With tile shaders, you can do threadgroups in a per-tile basis.

Tile dispatch can access prior fragments, and be used in next fragment.

How do we do this?

First, set `renderPassDescriptor.tileWidth` / `tileHeight` to match our scheme.  Ex, 32x32.  Keep in mind apple GPUs only support a few tile dimensions.

`renderPassDesc.threadgroupMemoryLength`.  

More on tile shading:
[[Metal 2 on A11 - tile shading - tech talks]]
[[Modern rendering with Metal]]

## Repurposing tile memory
[[Metal2 on a11]]

# Optimize for the apple GPU shader core

* Scalar ALU with vectorized load/store
* Constant execution and prefetch
* 16-bit and 32-bit ALU

## Address spaces

### Device
* Read-write
* No size restrictions

### Constant
* Readonly
* Limited-size
* Optimized for re-use of constant data


How to pick best address space?

* How much data are we dealing with?
	* If the amount of data is not known at compile time, and each draw/dispatch reads a different number of items, than the data should be placed in the device address space.
	* If the size is fixed, how many times is each item read?  If it's a few, use device.  If the same is used for various threads, used constant.  Generally, for things that are indexed by vertex, you want device.


## Constant buffer preloading

In some cases, the hardware can load your stuff in buffers in the constant space.

* Load offset known at compile-time
* Array size known at compile-time

Buffers tagged with the constant address space can likely be preloaded.  More likely if the offset is known at compile-time.  Indexing into an array, its size needs to be known at compile time as well.

Even if we specify the `const` qualifier, we're reading from `device` address space.  But also, the size of the array is not known at compile time.

To help the shader compiler, pass single struct arguments by reference.
Pass bounded arrays in a struct, rather than via a pointer.

## Use correct ALU data types
Optimized for 16-bit datatypes.  When you use a bigger type, you will allocate more registers.  Using 16-bit datatypes is preferred.  Leads to increased shader core occupancy.

In most cases, 16-bit datatypes do faster arithmetic.  Better ALU utilization.

Use half/short rather than float/int when possible.  Type conversions are typically free, even between float/half.

Avoid floating point literals when doing half-precision operations.  When writing half-precision code, use `h` suffix.

## Optimizing memory access
Avoid dynamically-indexed non-constant stack arrays.

Use GPU frame debugger in Xcode.  

Best practiced is to use signed types for memory addressing.  This is because `uint` is defined to wrap, and doing this is expensive.  In cases you don't need to wrap, `int` is preferred.

Batching memory accesses together is preferred.


[[gain insights into your metal app with xcode 12]]
[[Delivering optimized metal apps and games]]
[[Modern rendering with Metal]]