#avfoundation 

Goal: optmizing file to renderer

* leverage afterburner or hardware acceleration resources
* optimize pass for compressed and uncompressed frames

# video integration overview
A peek inside the box.  Decoders run in a sandboxed process.
This adds security
Increases stability.  If there's a crash in the decoder, the result is a decode error.

## frameworks overview

AVKit.  High-level options for dropping media functionality into an app.
AVFoundation.  Powerful and flexible interface for working with all aspects of media.
Video toolbox.  A low-level interface for working with video decoders / encoders.
Core Media - basic building box
Core Video - building blocks for video

|                        | Hardware accel (including afterburner) | CMSampleBuffers optimized for RPC |
|------------------------|----------------------------------------|-----------------------------------|
| # AVFoundation         |                                        |                                   |
| AVPlayer               | Automatic                              | Yes                               |
| AVAssetExportSession   | Automatic                              | Yes                               |
| AVAssetReader          | Automatic                              | Yes                               |
| # Video Toolbox        |                                        |                                   |
| VTDecompressionSession | Automatic                              | It depends                        |

If CMSampleBuffer are generated by AVFoundation, they're created in a form optimized for XPC (RPC)

## Video pipeline lexicon
* `CVPixelBuffer` -> wrapper around uncompressed raster image data.  Pixel format, height, width, row bytes.  Attachments which describe image datal ike color tags.
* `CMBlockBuffer` -> Usually compressed data, arbitrary blocks of data.
* `CMSampleBuffer` ->  one of
	* Can wrap a `CMBlockBuffer`.  Also contains `CMTime` and `CMFormatDesc`
	* Can wrap a `CVPixelBuffer`.  Also contains `CMTime` and `CMFormatDesc`
	* Marker.  No payload, exist entirely to carry timed attachments through a media pipleine signaling specific conditions.
* `IOSurface` -> The raster data in a `CVPixelBuffer` is usually in this form.  Can also be used as the basis for memory for a Metal texture (`MTLTexture`).  Allows memory to be efficiently moved between processes, memory, vram, gpus, etc.
* `CVPixelBufferPool` -> In most cases, contain multiple `IOSurface`.  Sort of a reuse system here, where released values are re-used.  Have a fixed pixel format, height, width  

# AVFoundation
* avassetreader
* AVSampleBufferGenerator

AVAssetReader does it all.
1.  Reads samples from source file, optimizing for RPC
2.  Decodes video data in the sandbox proces
3.  Provides decoded `CVPixelBuffer` in the requested output format.

```objc
// Constructing an AVAssetReader

// Create an AVAsset with an URL pointing at a local asset
AVAsset *sourceMovieAsset = [AVAsset assetWithURL:sourceMovieURL];

// Create an AVAssetReader for the asset
AVAssetReader *assetReader = [AVAssetReader assetReaderWithAsset:sourceMovieAsset 
                                                           error:&error];
```

```objc
// Configuring AVAssetReaderTrackOutput

// Copy the array of video tracks from the source movie
NSArray<AVAssetTrack*>  *tracks = [sourceMovieAsset tracksWithMediaType:AVMediaTypeVideo];
    
// Get the first video track
AVAssetTrack *track = [sourceMovieVideoTracks objectAtIndex:0];

// Create the asset reader track output for this video track, requesting ‘y416’ output
NSDictionary *outputSettings = @{ (id)kCVPixelBufferPixelFormatTypeKey :
                                  @(kCVPixelFormatType_4444AYpCbCr16) };

AVAssetReaderTrackOutput* assetReaderTrackOutput
= [AVAssetReaderTrackOutput assetReaderTrackOutputWithTrack:track
                                             outputSettings:outputSettings];

// Set the property to instruct the track output to return the samples 
// without copying them
assetReaderTrackOutput.alwaysCopiesSampleData = NO;
   
// Connect the the AVAssetReaderTrackOutput to the AVAssetReader
[assetReader addOutput:assetReaderTrackOutput];
```
```objc
// Running AVAssetReader

BOOL success = [assetReader startReading];

if (success) {
   CMSampleBufferRef sampleBuffer = NULL;
        
   // output is a AVAssetReaderOutput
   while ((sampleBuffer = [output copyNextSampleBuffer]))
   {
       CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
   //note that some sample buffers are "markers" and have no data?
       if (imageBuffer)
       {
          // Use the image buffer here
          // if imageBuffer is NULL, this is likely a marker sampleBuffer
       }
    }
}
```
AVAssetReader will convert, if you are requesting a format that the decode does not support.  But avoiding these copies will improve perf.

## How to avoid conversion?
|              (nil)       |                     (nil)             |
|---------------------|------------------------------------|
| ProRes 4444 formats | `kCVPixelFormatType_4444AYpCbCr16` |
| ProRes 422 formats  | `kCVPixelFormatType_422YpCbCr16`   |
| ProRes RAW formats  | `kCVPixelFormatType_64RGBAHalf`    |

# DIY: sourcing compressed data
AVAssetReaderSource data form a media file.  Use AVAssetReader and request compressed data.  This provides track level media access with awareness of edits, etc.

AVSampleBufferGenerator.  Media-level access to samples.
Custom CMSampleBuffer

## AVAssetReader
* reads samples from source asset
* Handles edits and frame dependencies to return required samples
* Prepares CMSampleBuffers for optimized RPC transfer

```objc
AVAssetReaderTrackOutput* assetReaderTrackOutput
= [AVAssetReaderTrackOutput assetReaderTrackOutputWithTrack:track
                                             outputSettings:nil];
```
## Using `AVSampleBufferGenerator` to generate samples
* Provides compressed samples read directly from the source asset
* Prepares `CMSampleBuffer`s for optimized RPC transfer

```objc
AVSampleCursor* cursor = [assetTrack makeSampleCursorAtFirstSampleInDecodeOrder];
        
AVSampleBufferRequest* request = [[AVSampleBufferRequest alloc] initWithStartCursor:cursor];
        
request.direction = AVSampleBufferRequestDirectionForward;
request.preferredMinSampleCount = 1;
request.maxSampleCount = 1;

//timebase: nil is sync
//in production, you probably want a timebase and async
AVSampleBufferGenerator* generator
= [[AVSampleBufferGenerator alloc] initWithAsset:srcAsset timebase:nil];

BOOL notDone = YES;
    
while(notDone)
{
   CMSampleBufferRef sampleBuffer = [generator createSampleBufferForRequest:request];

   // do your thing with the sampleBuffer

   [cursor stepInDecodeOrderByCount:1];
}
```

## Generating your own `CMSampleBuffer`s
* does not allow optimization for sharing `CMSampleBuffer`s over the sandbox RPC
* You provide the source data in a `CMBlockBuffer`
* Construct a `CMSampleBuffer` to wrap the `CMBlockBuffer`

```objc
CMBlockBufferCreateWithMemoryBlock(kCFAllocatorDefault, sampleData, sizeof(sampleData), 
                                   kCFAllocatorMalloc, NULL, 0, sizeof(sampleData), 0, 
                                   &blockBuffer);

CMVideoFormatDescriptionCreate(kCFAllocatorDefault, kCMVideoCodecType_AppleProRes4444, 1920, 
                               1080, extensionsDictionary, &formatDescription);

CMSampleTimingInfo timingInfo;

timingInfo.duration = CMTimeMake(10, 600);
timingInfo.presentationTimeStamp = CMTimeMake(frameNumber * 10, 600);

CMSampleBufferCreateReady(kCFAllocatorDefault, blockBuffer, formatDescription, 1, 1, 
                          &timingInfo, 1, &sampleSize, &sampleBuffer);
```

# DIY: Using `VTDecompressionSession`
* video decoder (seperate sandbox process)
* `CVPixelBufferPool`
* Possibly, a `VTPixeTransferSession` to convert

[optional] call `VTRegisterProfessionalVideoWorkflowDecoders` once?
1.  Create the `VTDecompressionSession`
2.  Configure session if desired with `VTSessionSetProperty` calls
3.  Begin sending frames to the `VTDecompressionSession` to be decoded

`videoFormatDescription` describes incoming source data
`destinationImageBufferAttributes` describe output pixelBuffer requirements. e.g., if you only can consume certain types, you can request that.  Or can request just to provide CA-compatible output
`videoDecoderSpecfication` provides hints about codec selection.  Like don't use hardware decoder.

In current OS, all hardware-accelerated codecs are enabled by default.  If you want to guarantee hardware decoder,

`kVTVideoDecoderSpecification_RequireHardwareAcceleratedVideoDecoder`

or to require software,

`kVTVideoDecoderSpecification_EnableHardwareAcceleratedVideoDecoder` -> false

Note these keys are different.

```objc
// VTDecompressionSession Creation

CMFormatDescriptionRef formatDesc = CMSampleBufferGetFormatDescription(sampleBuffer);

//if we want to request a specific output format
CFDictionaryRef pixelBufferAttributes = (__bridge CFDictionaryRef)@{
    (id)kCVPixelBufferPixelFormatTypeKey :
    @(kCVPixelFormatType_4444AYpCbCr16) };

VTDecompressionSessionRef decompressionSession;
    
//null for video decoder spec, means VT will do its default hardware decoder selection.
OSStatus err = VTDecompressionSessionCreate(kCFAllocatorDefault, 
                                            formatDesc, 
                                            NULL,
                                            pixelBufferAttributes, 
                                            NULL, 
                                            &decompressionSession);
```

```objc
// Running a VTDecompressionSession

uint32_t inFlags = kVTDecodeFrame_EnableAsynchronousDecompression;

VTDecompressionOutputHandler  outputHandler
 = ^(OSStatus status,
     VTDecodeInfoFlags infoFlags,
     CVImageBufferRef imageBuffer,
     CMTime presentationTimeStamp,
     CMTime presentationDurationVTDecodeInfoFlags)
 {
     // Handle decoder output in this block
     // Status reports any decoder errors
     // imageBuffer contains the decoded frame if there were no errors
 };

VTDecodeInfoFlags outFlags;

OSStatus err = VTDecompressionSessionDecodeFrameWithOutputHandler(decompressionSession,
                                                   sampleBuffer, inFlags, 
                                                   &outFlags, outputHandler);
```

Notes on decompression output
* decompression output is serialized.  Only 1 frame at a time.
* If you block inside the decoder output, it will block subsequent frames
* ultimately causing backpressure in the decoder
* expensive processing work should be done outside the callback

# Using `CVPixelBuffer`s with metal
corevideo

How does the pool work?
When released, the IOSurface goes back in the pool.  The `IOSurface` is then recycled and used for the new `CVPixelBuffer`.

We need to ensure that `IOSurface` is not recycled while in use by metal.

## Approaches to integrating `CVPixelBuffer` with Metal

* Direct IOSurface usage.  Appears simple, but a trick to ensure the iosurface is not recycled while in use by metal
* `CVMetalTextureCache`.  Generally simpler to use safely.


```objc
// CVPixelBuffer to Metal texture: IOSurface

IOSurfaceRef surface = CVPixelBufferGetIOSurface(imageBuffer);

id <MTLTexture> metalTexture = [metalDevice newTextureWithDescriptor:descriptor
                                                           iosurface:surface 
                                                               plane:0];

// Mark the IOSurface as in-use so that it won’t be recycled by the CVPixelBufferPool
IOSurfaceIncrementUseCount(surface);

// Set up command buffer completion handler to decrement IOSurface use count again
[cmdBuffer addCompletedHandler:^(id<MTLCommandBuffer> buffer) {
     IOSurfaceDecrementUseCount(surface);
 }];
```

```objc
// Create a CVMetalTextureCacheRef

CVMetalTextureCacheRef metalTextureCache = NULL;

id <MTLDevice> metalDevice = MTLCreateSystemDefaultDevice();
    
CVMetalTextureCacheCreate(kCFAllocatorDefault, NULL, metalDevice, NULL, &metalTextureCache);

// Create a CVMetalTextureRef using metalTextureCache and our pixelBuffer
CVMetalTextureCacheCreateTextureFromImage(kCFAllocatorDefault,
                                          metalTextureCache,
                                          pixelBuffer,
                                          NULL,
                                          pixelFormat,
                                          CVPixelBufferGetWidth(pixelBuffer),
                                          CVPixelBufferGetHeight(pixelBuffer),
                                          0,
                                          &cvTexture);

id <MTLTexture>  texture = CVMetalTextureGetTexture(cvTexture);
// Be sure to release the cvTexture object when the Metal command buffer completes!
```
# Wrap up
* ensuring use of afterburner and other hardware decoders
* Using `AVAssetReader` for simple integration of video into your app
* How to create samples and use Video Toolbox if needed
* How to integrate `CVPixelBuffer`s with metal

