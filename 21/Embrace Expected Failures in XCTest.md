#xctest 

At a high level, testing is how we ensure the quality of the product.  But also to discover bugs before we ship.

Testing is an investment.  Create, run, maintain tests.  We want to maximize returns while minimizing costs.

Tools for reducing the maintenance cost.  How to handle failures when they occur?

# New failures are valuable
Indicates flaw in the product, flaw in test, or some issue in one of the dependencies.  Frameworks, subsystems, etc.

Once that failure has been registered, subsequent reports of the same failure are less valuable.  Represent information you already have.

Ideally, any new failure is triaged and fixed quickly.  But you may not be able to resolve the problem right away.

# Known failures are noisy

What tools are available for managing the noise?  
* Disabling
* XCTSkip
* Expected failures

# Disabling
Test will continue to be compiled, but won't execute.  Won't see it in the test report.  Harder to track as an issue that needs to be resolved.

Ability to choose enabled/disabled, curating collections of tests for specific purposes.

# XCTSkip
With this approach, the code builds.  It also executes up until `XCTSkip` is called.  So it's included in the test report, giving better visibility into the issue.

However, it doesn't execute all of your tests.  You lose out on potentially-useful information in the form of new issues and changes to existing issue.

Requiring a specific version or device type.

```swift
try XCTSkipUnless(UIDevice.current.userInterfaceIdiom == .pad, "Only supported on iPad")
```

# XCTExpectFailure

Failure in the test will be reported as an expected failure.

Failure in the suite will be reported as a pass.

Eliminates noise generated by the failure, making it easy to see if there are other issues in your test.

Suppressing the noise doesn't solve the underlying issue, so the API takes a failure reason.  Documents the problem in your code.

Green "dash" means the suite passed in a mixed state.

## Using XCTexpectFailure

* Stateful approach.  Call `XCTExpectFailure` and any subsequent failure is expected
* Scoped approach.  Wrap failing code in a closure.

Function is no longer returning true.  Scoped approach

```swift
XCTExpectFailure("<https://dev.myco.com/bugs/4923> fix myValidationFunction") {
    XCTAssert(myValidationFunction())
}
```

API supports nesting.  Can call API more than once in a test, including inside the closure from another call.  Important consideration using in library code.

e.g. if a common utility function fails, many tests may be impacted, some of which may already be using the API.

For nested calls, issue is matched against nearest callsite.  If rejected, will be passed to next call, stack semantics.

With shared code, it's best to use the scoped variant to limit the effects on test state.

## Issue matching
Can be more selective.

```swift
let options = XCTExpectedFailure.Options()
options.issueMatcher = { issue in
    return issue.type == .assertionFailure
}

XCTExpectFailure("<https://dev.myco.com/bugs/4923> fix myValidationFunction", options: options)
```

My test may be passing on iOS and failing on MacoS.  

```swift
let options = XCTExpectedFailure.Options()
#if os(macOS)
options.isEnabled = false
#endif

XCTExpectFailure("<https://dev.myco.com/bugs/4923> fix myValidationFunction", options: options) {
    XCTAssert(myValidationFunction())
}
```

When expected failure stops failing?  

If you're still calling the API without failure, it will generate a *new* failure.  We call this an *unmatched* expected failure.

XCTExpectFailure is "strict".  Helps you remove unnecessary calls to the API.

 
| Deterministic | Nondeterministic |
|---------------|------------------|
| Environment   | Timing           |
| Device type   | Ordering         |
| OS version    | Concurrency      |

For nondeterministic failures, "strict" behavior isn't helpful.

```swift
let options = XCTExpectedFailure.Options()
options.isStrict = false

XCTExpectFailure("<https://dev.myco.com/bugs/4923> fix myValidationFunction", options: options) {
    XCTAssert(myValidationFunction())
}
```

alternatively

```swift
XCTExpectFailure("<https://dev.myco.com/bugs/4923> fix myValidationFunction", strict: false) {
    XCTAssert(myValidationFunction())
}
```

Great way to handle "flaky" nondeterministic tests.

## Investigating
* Test repetitions in xcode

[[Diagnose unreliable code with test repetitions]]


